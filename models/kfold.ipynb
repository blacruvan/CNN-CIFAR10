{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from networks import *\n",
    "from images_compose import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformConfig()\n",
    "composed_train = config.composed_train\n",
    "composed_test = config.composed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "#dataset = datasets.CIFAR10(root='mnist_data', download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset =  datasets.CIFAR10(root='./data', train=True, download=True, transform = composed_train)\n",
    "validation_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform = composed_test)\n",
    "\n",
    "# Create train and validation batch for training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, validation_loader, n_epochs = 100):\n",
    "    \n",
    "    # Global variable\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    learning_rate = 0.1\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    N_test = len(validation_dataset)\n",
    "    accuracy_list = []\n",
    "    train_loss_list = []\n",
    "    model = model.to(device)\n",
    "    train_cost_list = []\n",
    "    val_cost_list = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_COST = 0\n",
    "        for x,y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x)\n",
    "            loss = criterion(z,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_COST+=loss.item()\n",
    "            \n",
    "        train_COST = train_COST/len(train_loader)\n",
    "        train_cost_list.append(train_COST)\n",
    "        correct = 0\n",
    "        \n",
    "        # Perform the prediction on the validation data\n",
    "        val_COST = 0\n",
    "        for x_test, y_test in validation_loader:\n",
    "            model.eval()\n",
    "            x_test = x_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "            z = model(x_test)\n",
    "            val_loss = criterion(z, y_test)\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            correct += (yhat==y_test).sum().item()\n",
    "            val_COST+=val_loss.item()\n",
    "        \n",
    "        val_COST = val_COST/ len(validation_loader)\n",
    "        val_cost_list.append(val_COST)\n",
    "            \n",
    "        accuracy = correct / N_test\n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "        print(\"--> Epoch Number : {}\".format(epoch + 1),\n",
    "            \" | Training Loss : {}\".format(round(train_COST,4)),\n",
    "            \" | Validation Loss : {}\".format(round(val_COST,4)),\n",
    "              \" | Validation Accuracy : {}%\".format(round(accuracy * 100, 2)))\n",
    "        \n",
    "    return accuracy_list, train_cost_list, val_cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch Number : 1  | Training Loss : 1.6933  | Validation Loss : 1.4779  | Validation Accuracy : 46.99%\n",
      "--> Epoch Number : 2  | Training Loss : 1.4329  | Validation Loss : 1.3246  | Validation Accuracy : 52.75%\n",
      "--> Epoch Number : 3  | Training Loss : 1.3402  | Validation Loss : 1.2481  | Validation Accuracy : 55.66%\n",
      "--> Epoch Number : 4  | Training Loss : 1.2901  | Validation Loss : 1.197  | Validation Accuracy : 57.57%\n",
      "--> Epoch Number : 5  | Training Loss : 1.2502  | Validation Loss : 1.1685  | Validation Accuracy : 58.29%\n",
      "--> Epoch Number : 6  | Training Loss : 1.2154  | Validation Loss : 1.1756  | Validation Accuracy : 58.28%\n",
      "--> Epoch Number : 7  | Training Loss : 1.1884  | Validation Loss : 1.1216  | Validation Accuracy : 60.67%\n",
      "--> Epoch Number : 8  | Training Loss : 1.1635  | Validation Loss : 1.1074  | Validation Accuracy : 60.8%\n",
      "--> Epoch Number : 9  | Training Loss : 1.1411  | Validation Loss : 1.0765  | Validation Accuracy : 62.24%\n",
      "--> Epoch Number : 10  | Training Loss : 1.1244  | Validation Loss : 1.0662  | Validation Accuracy : 62.56%\n",
      "--> Epoch Number : 11  | Training Loss : 1.1055  | Validation Loss : 1.0662  | Validation Accuracy : 62.52%\n",
      "--> Epoch Number : 12  | Training Loss : 1.0875  | Validation Loss : 1.0849  | Validation Accuracy : 62.19%\n",
      "--> Epoch Number : 13  | Training Loss : 1.0751  | Validation Loss : 1.049  | Validation Accuracy : 63.38%\n",
      "--> Epoch Number : 14  | Training Loss : 1.0592  | Validation Loss : 1.0575  | Validation Accuracy : 62.95%\n",
      "--> Epoch Number : 15  | Training Loss : 1.0473  | Validation Loss : 1.0495  | Validation Accuracy : 63.74%\n",
      "--> Epoch Number : 16  | Training Loss : 1.0381  | Validation Loss : 1.058  | Validation Accuracy : 63.02%\n",
      "--> Epoch Number : 17  | Training Loss : 1.024  | Validation Loss : 1.0583  | Validation Accuracy : 63.11%\n",
      "--> Epoch Number : 18  | Training Loss : 1.0192  | Validation Loss : 1.0378  | Validation Accuracy : 63.75%\n",
      "--> Epoch Number : 19  | Training Loss : 1.008  | Validation Loss : 1.0321  | Validation Accuracy : 64.01%\n",
      "--> Epoch Number : 20  | Training Loss : 0.9986  | Validation Loss : 1.0475  | Validation Accuracy : 63.73%\n",
      "--> Epoch Number : 21  | Training Loss : 0.9913  | Validation Loss : 1.0673  | Validation Accuracy : 63.39%\n",
      "--> Epoch Number : 22  | Training Loss : 0.984  | Validation Loss : 1.0381  | Validation Accuracy : 64.31%\n",
      "--> Epoch Number : 23  | Training Loss : 0.9734  | Validation Loss : 1.0234  | Validation Accuracy : 64.74%\n",
      "--> Epoch Number : 24  | Training Loss : 0.9645  | Validation Loss : 1.0258  | Validation Accuracy : 64.49%\n",
      "--> Epoch Number : 25  | Training Loss : 0.9584  | Validation Loss : 1.0412  | Validation Accuracy : 64.05%\n",
      "--> Epoch Number : 26  | Training Loss : 0.9513  | Validation Loss : 1.0551  | Validation Accuracy : 64.12%\n",
      "--> Epoch Number : 27  | Training Loss : 0.9506  | Validation Loss : 1.0094  | Validation Accuracy : 64.8%\n",
      "--> Epoch Number : 28  | Training Loss : 0.9404  | Validation Loss : 1.0458  | Validation Accuracy : 64.56%\n",
      "--> Epoch Number : 29  | Training Loss : 0.9334  | Validation Loss : 1.0443  | Validation Accuracy : 64.37%\n",
      "--> Epoch Number : 30  | Training Loss : 0.9283  | Validation Loss : 1.0674  | Validation Accuracy : 63.65%\n",
      "--> Epoch Number : 31  | Training Loss : 0.9322  | Validation Loss : 1.0326  | Validation Accuracy : 64.97%\n",
      "--> Epoch Number : 32  | Training Loss : 0.917  | Validation Loss : 1.0266  | Validation Accuracy : 65.46%\n",
      "--> Epoch Number : 33  | Training Loss : 0.9129  | Validation Loss : 1.0289  | Validation Accuracy : 64.88%\n",
      "--> Epoch Number : 34  | Training Loss : 0.9087  | Validation Loss : 1.0354  | Validation Accuracy : 65.16%\n",
      "--> Epoch Number : 35  | Training Loss : 0.901  | Validation Loss : 1.0311  | Validation Accuracy : 65.28%\n",
      "--> Epoch Number : 36  | Training Loss : 0.8921  | Validation Loss : 1.0404  | Validation Accuracy : 65.3%\n",
      "--> Epoch Number : 37  | Training Loss : 0.893  | Validation Loss : 1.0345  | Validation Accuracy : 65.41%\n",
      "--> Epoch Number : 38  | Training Loss : 0.8862  | Validation Loss : 1.0429  | Validation Accuracy : 65.54%\n",
      "--> Epoch Number : 39  | Training Loss : 0.8823  | Validation Loss : 1.0441  | Validation Accuracy : 65.01%\n",
      "--> Epoch Number : 40  | Training Loss : 0.8834  | Validation Loss : 1.0144  | Validation Accuracy : 66.14%\n",
      "--> Epoch Number : 41  | Training Loss : 0.8737  | Validation Loss : 1.0421  | Validation Accuracy : 65.44%\n",
      "--> Epoch Number : 42  | Training Loss : 0.8719  | Validation Loss : 1.0429  | Validation Accuracy : 65.5%\n",
      "--> Epoch Number : 43  | Training Loss : 0.8698  | Validation Loss : 1.0323  | Validation Accuracy : 65.62%\n",
      "--> Epoch Number : 44  | Training Loss : 0.863  | Validation Loss : 1.0089  | Validation Accuracy : 66.53%\n",
      "--> Epoch Number : 45  | Training Loss : 0.8554  | Validation Loss : 1.0079  | Validation Accuracy : 66.19%\n",
      "--> Epoch Number : 46  | Training Loss : 0.8495  | Validation Loss : 1.0542  | Validation Accuracy : 65.67%\n",
      "--> Epoch Number : 47  | Training Loss : 0.8565  | Validation Loss : 1.0193  | Validation Accuracy : 66.16%\n",
      "--> Epoch Number : 48  | Training Loss : 0.8429  | Validation Loss : 1.0334  | Validation Accuracy : 66.23%\n",
      "--> Epoch Number : 49  | Training Loss : 0.8461  | Validation Loss : 1.0203  | Validation Accuracy : 66.61%\n",
      "--> Epoch Number : 50  | Training Loss : 0.8399  | Validation Loss : 1.0475  | Validation Accuracy : 65.8%\n",
      "--> Epoch Number : 51  | Training Loss : 0.8426  | Validation Loss : 1.0323  | Validation Accuracy : 65.68%\n",
      "--> Epoch Number : 52  | Training Loss : 0.8306  | Validation Loss : 1.0603  | Validation Accuracy : 65.33%\n",
      "--> Epoch Number : 53  | Training Loss : 0.83  | Validation Loss : 1.0461  | Validation Accuracy : 66.21%\n",
      "--> Epoch Number : 54  | Training Loss : 0.8287  | Validation Loss : 1.0367  | Validation Accuracy : 66.06%\n",
      "--> Epoch Number : 55  | Training Loss : 0.8202  | Validation Loss : 1.0681  | Validation Accuracy : 66.12%\n",
      "--> Epoch Number : 56  | Training Loss : 0.818  | Validation Loss : 1.0373  | Validation Accuracy : 66.27%\n",
      "--> Epoch Number : 57  | Training Loss : 0.8148  | Validation Loss : 1.0679  | Validation Accuracy : 65.89%\n",
      "--> Epoch Number : 58  | Training Loss : 0.8225  | Validation Loss : 1.0508  | Validation Accuracy : 66.45%\n",
      "--> Epoch Number : 59  | Training Loss : 0.8143  | Validation Loss : 1.0686  | Validation Accuracy : 65.93%\n",
      "--> Epoch Number : 60  | Training Loss : 0.8104  | Validation Loss : 1.0617  | Validation Accuracy : 65.88%\n",
      "--> Epoch Number : 61  | Training Loss : 0.8135  | Validation Loss : 1.0212  | Validation Accuracy : 67.16%\n",
      "--> Epoch Number : 62  | Training Loss : 0.8046  | Validation Loss : 1.0374  | Validation Accuracy : 66.79%\n",
      "--> Epoch Number : 63  | Training Loss : 0.8036  | Validation Loss : 1.0724  | Validation Accuracy : 65.99%\n",
      "--> Epoch Number : 64  | Training Loss : 0.7933  | Validation Loss : 1.0653  | Validation Accuracy : 66.57%\n",
      "--> Epoch Number : 65  | Training Loss : 0.7988  | Validation Loss : 1.0471  | Validation Accuracy : 66.83%\n",
      "--> Epoch Number : 66  | Training Loss : 0.7944  | Validation Loss : 1.0492  | Validation Accuracy : 66.34%\n",
      "--> Epoch Number : 67  | Training Loss : 0.7947  | Validation Loss : 1.053  | Validation Accuracy : 66.13%\n",
      "--> Epoch Number : 68  | Training Loss : 0.7876  | Validation Loss : 1.0474  | Validation Accuracy : 66.61%\n",
      "--> Epoch Number : 69  | Training Loss : 0.7852  | Validation Loss : 1.0516  | Validation Accuracy : 66.69%\n",
      "--> Epoch Number : 70  | Training Loss : 0.7891  | Validation Loss : 1.04  | Validation Accuracy : 66.65%\n",
      "--> Epoch Number : 71  | Training Loss : 0.7786  | Validation Loss : 1.073  | Validation Accuracy : 66.2%\n",
      "--> Epoch Number : 72  | Training Loss : 0.7814  | Validation Loss : 1.0671  | Validation Accuracy : 66.59%\n",
      "--> Epoch Number : 73  | Training Loss : 0.775  | Validation Loss : 1.0818  | Validation Accuracy : 66.41%\n",
      "--> Epoch Number : 74  | Training Loss : 0.7759  | Validation Loss : 1.0514  | Validation Accuracy : 67.12%\n",
      "--> Epoch Number : 75  | Training Loss : 0.7769  | Validation Loss : 1.0882  | Validation Accuracy : 66.03%\n",
      "--> Epoch Number : 76  | Training Loss : 0.7688  | Validation Loss : 1.0659  | Validation Accuracy : 67.32%\n",
      "--> Epoch Number : 77  | Training Loss : 0.7732  | Validation Loss : 1.0751  | Validation Accuracy : 66.6%\n",
      "--> Epoch Number : 78  | Training Loss : 0.7644  | Validation Loss : 1.047  | Validation Accuracy : 66.83%\n",
      "--> Epoch Number : 79  | Training Loss : 0.7647  | Validation Loss : 1.0593  | Validation Accuracy : 66.7%\n",
      "--> Epoch Number : 80  | Training Loss : 0.7639  | Validation Loss : 1.1245  | Validation Accuracy : 65.54%\n",
      "--> Epoch Number : 81  | Training Loss : 0.761  | Validation Loss : 1.0706  | Validation Accuracy : 66.67%\n",
      "--> Epoch Number : 82  | Training Loss : 0.7547  | Validation Loss : 1.0566  | Validation Accuracy : 67.01%\n",
      "--> Epoch Number : 83  | Training Loss : 0.7569  | Validation Loss : 1.0544  | Validation Accuracy : 66.89%\n",
      "--> Epoch Number : 84  | Training Loss : 0.7511  | Validation Loss : 1.0595  | Validation Accuracy : 67.34%\n",
      "--> Epoch Number : 85  | Training Loss : 0.752  | Validation Loss : 1.0807  | Validation Accuracy : 66.74%\n",
      "--> Epoch Number : 86  | Training Loss : 0.7489  | Validation Loss : 1.0716  | Validation Accuracy : 67.11%\n",
      "--> Epoch Number : 87  | Training Loss : 0.7441  | Validation Loss : 1.0975  | Validation Accuracy : 66.41%\n",
      "--> Epoch Number : 88  | Training Loss : 0.7425  | Validation Loss : 1.1618  | Validation Accuracy : 64.88%\n",
      "--> Epoch Number : 89  | Training Loss : 0.7491  | Validation Loss : 1.0968  | Validation Accuracy : 66.26%\n",
      "--> Epoch Number : 90  | Training Loss : 0.7451  | Validation Loss : 1.0842  | Validation Accuracy : 66.47%\n",
      "--> Epoch Number : 91  | Training Loss : 0.7428  | Validation Loss : 1.097  | Validation Accuracy : 66.85%\n",
      "--> Epoch Number : 92  | Training Loss : 0.7457  | Validation Loss : 1.0979  | Validation Accuracy : 66.82%\n",
      "--> Epoch Number : 93  | Training Loss : 0.7371  | Validation Loss : 1.08  | Validation Accuracy : 66.94%\n",
      "--> Epoch Number : 94  | Training Loss : 0.7429  | Validation Loss : 1.0953  | Validation Accuracy : 66.45%\n",
      "--> Epoch Number : 95  | Training Loss : 0.7365  | Validation Loss : 1.1036  | Validation Accuracy : 66.48%\n",
      "--> Epoch Number : 96  | Training Loss : 0.7351  | Validation Loss : 1.1019  | Validation Accuracy : 66.63%\n",
      "--> Epoch Number : 97  | Training Loss : 0.7367  | Validation Loss : 1.0837  | Validation Accuracy : 66.86%\n",
      "--> Epoch Number : 98  | Training Loss : 0.729  | Validation Loss : 1.1479  | Validation Accuracy : 65.28%\n",
      "--> Epoch Number : 99  | Training Loss : 0.7285  | Validation Loss : 1.0927  | Validation Accuracy : 66.58%\n",
      "--> Epoch Number : 100  | Training Loss : 0.7273  | Validation Loss : 1.0891  | Validation Accuracy : 67.06%\n"
     ]
    }
   ],
   "source": [
    "model = NN02()\n",
    "accuracy_list_normal, train_cost_list, val_cost_list = train_model(model, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def KFold_Model(model, dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform = composed_train)):\n",
    "  \n",
    "  k = 10\n",
    "  kf = KFold(n_splits=k, random_state=56, shuffle=True)\n",
    "  accuracies = []\n",
    "  torch.manual_seed(56)\n",
    "\n",
    "  for train_idx, test_idx in kf.split(dataset):\n",
    "      train_loader = DataLoader(Subset(dataset, train_idx), batch_size=100, shuffle=True)\n",
    "      test_loader = DataLoader(Subset(dataset, test_idx), batch_size=100, shuffle=False)\n",
    "\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "      model = model.to(device)\n",
    "      # Training\n",
    "      for epoch in range(50):\n",
    "          model.train()\n",
    "          for images, labels in train_loader:\n",
    "              images, labels = images.to(device), labels.to(device)\n",
    "              optimizer.zero_grad()\n",
    "              outputs = model(images)\n",
    "              loss = criterion(outputs, labels)\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "\n",
    "      # Evaluation\n",
    "      model.eval()\n",
    "      correct, total = 0, 0\n",
    "      with torch.no_grad():\n",
    "          for images, labels in test_loader:\n",
    "              images, labels = images.to(device), labels.to(device)\n",
    "              outputs = model(images)\n",
    "              _, predicted = torch.max(outputs, 1)\n",
    "              total += labels.size(0)\n",
    "              correct += (predicted == labels).sum().item()\n",
    "\n",
    "      accuracy = 100 * correct / total\n",
    "      accuracies.append(accuracy)\n",
    "      print(f'Fold Accuracy: {accuracy}%')\n",
    "\n",
    "  print(f'Mean Accuracy: {np.mean(accuracies)}%')\n",
    "  print(f'Standard Deviation: {np.std(accuracies)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Accuracy: 62.8%\n",
      "Fold Accuracy: 64.52%\n",
      "Fold Accuracy: 69.52%\n",
      "Fold Accuracy: 70.74%\n",
      "Fold Accuracy: 71.94%\n",
      "Fold Accuracy: 73.46%\n",
      "Fold Accuracy: 75.8%\n",
      "Fold Accuracy: 77.06%\n",
      "Fold Accuracy: 75.92%\n",
      "Fold Accuracy: 77.76%\n",
      "Mean Accuracy: 71.95199999999998%\n",
      "Standard Deviation: 4.889050623587367%\n"
     ]
    }
   ],
   "source": [
    "model = NN02()\n",
    "KFold_Model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
